#!/bin/bash
#
# Kernel Patch tester to run some generic kernel static checks
#
# Dec 14, 2013
#
# Copyright (C) 2013 Texas Instruments Incorporated - http://www.ti.com/
#	Nishanth Menon
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License version 2 as
# published by the Free Software Foundation.
#
# This program is distributed "as is" WITHOUT ANY WARRANTY of any
# kind, whether express or implied; without even the implied warranty
# of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# NOTE: smatch: (sudo apt-get install sqlite3 libsqlite3-dev llvm)
#  - http://linuxplumbersconf.com/2011/ocw//system/presentations/165/original/transcript.txt
#  - http://smatch.sf.net
# spatch is provided by the coccinelle package in ubuntu

DEF_ARCH=x86_64
DEF_CROSS_COMPILE=
DEF_BUILDTARGETS="zImage dtbs"

# Default parameters
APPS_NEEDED="perl make ${CROSS_COMPILE}gcc sparse patch git realpath basename" 

# Use all max num CPUs 
DEF_CPUS=`grep -c '^processor' /proc/cpuinfo`
DEFAULT_LOG="./report-kernel-patch-verify.txt"
DEFAULT_TMPDIR="/tmp"

COVER_LETTER="cover-letter.[patch\|diff]$"

LINE_LENGTH=80

UBOOT_TESTING=0
# We will add to this later.. but allow user to provide his own choice of stuff
if [ -z "$KP_PARAMS" ]; then
	KP_PARAMS=""
fi
if [ -z "$KP_TARGETS" ]; then
	KP_TARGETS=""
fi

kmake_single() {
	make $KP_PARAMS -j1 $*
}

kmake() {
	make $KP_PARAMS -j$CPUS $*
}

###################
# Run generic test operation
run_test() {
	LOG_EXT=$1
	shift
	LOG_DIR=$1
	shift
	TEST=$1
	shift
	echo -e "\tRunning test: $TEST ($LOG_EXT)"
	SSEC=`date "+%s"`
	eval $TEST $* 2>$LOG_DIR/$TEST-$LOG_EXT
	ESEC=`date "+%s"`
	DELTA=`expr $ESEC - $SSEC`
	echo "$DELTA seconds: completed $TEST"
}

run_test_dummy() {
	LOG_EXT=$1
	shift
	LOG_DIR=$1
	shift
	TEST=$1
	shift
	echo -e "\tRunning test: $TEST ($LOG_EXT)"
	touch $LOG_DIR/$TEST-$LOG_EXT
}

get_sorted_existing_files()
{
	test_files=""
	for i in `(ls -d $* 2>/dev/null) | sort`
	do
		if [ -f "$i" ]; then
			test_files="$test_files $i"
		fi
	done
	echo $test_files
}

###################
# Basic tests to run on the patch itself
ptest_am() {
	git am $1 >/dev/null
}

ptest_check() {
	($KDIR/scripts/checkpatch.pl --strict $1 --max-line-length=$LINE_LENGTH |grep -v `basename $1`|grep -v "^$"|grep -v "^total"|grep -v "^NOTE:")1>&2
}

###################
# Basic tests to run on the files impacted by the patch
ftest_check_kdoc() {
	test_files=`get_sorted_existing_files $*`

	if [ -n "$test_files" ]; then
		((($KDIR/scripts/kernel-doc $test_files >/dev/null) 2>&1)|cut -d ':' -f1,3-) 1>&2
	fi
}

ftest_check_includes() {
	test_files=`get_sorted_existing_files $*`
	if [ -n "$test_files" ]; then
		$KDIR/scripts/checkincludes.pl $test_files 1>&2
	fi
}

ftest_check_headerdeps() {
	test_files=`get_sorted_existing_files $*`
	if [ -n "$test_files" ]; then
		$KDIR/scripts/headerdep.pl $test_files 1>&2
	fi
}

###################
# Basic build test
btest_basic() {
	test_files=`get_sorted_existing_files $*`
	if [ -n "$test_files" ]; then
		rm $test_files
		(((kmake_single $test_files > /dev/null) 2>&1)|cut -d ':' -f1,4- | grep -v "^make$") 1>&2
	fi
}

btest_sparse() {
	test_files=`get_sorted_existing_files $*`
	if [ -n "$test_files" ]; then
		(((kmake_single C=2 $test_files > /dev/null) 2>&1)|cut -d ':' -f1,4-) |grep -v "^mv$" |grep -v "^make$" | grep -v "__ksymtab" 1>&2
	fi
}

btest_smatch() {
	test_files=`get_sorted_existing_files $*`
	if [ -n "$test_files" ]; then
		kmake_single CHECK="$SMATCH" C=2 $test_files | egrep '(warn|error):' 1>&2
	fi
}

btest_cocci() {
	test_files=`get_sorted_existing_files $*`
	if [ -n "$test_files" ]; then
		kmake_single C=2 CHECK="scripts/coccicheck" MODE=report $test_files >/dev/null
	fi
}

btest_stack() {
	kmake checkstack 1>&2
}

btest_namespace() {
	kmake namespacecheck 1>&2
}

btest_include() {
	kmake_single includecheck 1>&2
}

btest_headers_check() {
	kmake_single headers_check 1>&2
}

btest_kbuild() {
	if [ $UBOOT_TESTING -eq 0 ]; then
		kmake C=1 $KP_TARGETS $MODULES>/dev/null
	else
		kmake C=1 $KP_TARGETS $MODULES>/dev/null 2>$TEST_DIR/err_stuff
		# Get rid of standard sparse mess with u-boot
		cat $TEST_DIR/err_stuff|grep -v "efi.h"|grep -v "version.c" |grep -v "_u_boot_list_" 1>&2
	fi
}

defconfig() {
	if [ -n "$DEFCONFIG" ]; then
		kmake $DEFCONFIG >/dev/null
	else
		cp $TEST_DIR/.config .config
		kmake oldconfig >/dev/null
	fi
}

build_all_clean() {
	(kmake clean 2>/dev/null )>/dev/null
	kmake_single $KP_TARGETS $MODULES >/dev/null
}
build_all() {
	(kmake $KP_TARGETS $MODULES 2>/dev/null) >/dev/null
}

# executed in sequence
tests_start() {
	echo "Running start tests.."
	TESTS_ALL_SET="defconfig"
	if [ -n "$COMPLETE_TESTS" ]; then
		TESTS_ALL_SET="$TESTS_ALL_SET build_all"
		if [ $UBOOT_TESTING -eq 0 ]; then
			TESTS_ALL1_SET="btest_stack btest_namespace btest_include btest_headers_check"
		fi
	fi
	echo "Sequential tests to run: $TESTS_ALL_SET"
	echo "Parallel tests to run: $TESTS_ALL1_SET"

	for test_s in $TESTS_ALL_SET
	do
		run_test start $TEST_DIR $test_s
	done
	# Run parallel tests
	PIDS=""
	for test_s in $TESTS_ALL1_SET
	do
		run_test start $TEST_DIR $test_s &
		PIDS="$PIDS $!"
	done
	echo "Waiting for PIDs: $PIDS"
	for pid in $PIDS
	do
		wait $pid
	done
	PIDS=""

	if [ -n "$COMPLETE_TESTS" ]; then
		build_all
	fi
}

tests_end() {
	echo "Running END tests.."
	for test_s in $TESTS_ALL_SET
	do
		run_test end $TEST_DIR $test_s
	done

	# Run parallel tests
	PIDS=""
	for test_s in $TESTS_ALL1_SET
	do
		run_test start $TEST_DIR $test_s &
		PIDS="$PIDS $!"
	done
	echo "Waiting for PIDs: $PIDS"
	for pid in $PIDS
	do
		wait $pid
	done
	PIDS=""
}

report_tests_end() {
	log_marker "::Complete test results START::"
	echo -en "\nGeneral Tests: ">>$LOG_SUMMARY_FILE
	report_tests $TESTS_ALL_SET $TESTS_ALL1_SET
	log_marker "::Complete test results END::"
}

test_patch() {
	patch=$1
	cfiles=`diffstat -lp1 $patch|grep -P "\.c$"|sort`
	ofiles=`diffstat -lp1 $patch|grep -P "\.[Sc]$"|sort|sed -e "s/[Sc]$/o/g"`

	# Run sequential tests
	TESTS_P_SET="ptest_am ptest_check"
	TESTS_B_SET="btest_basic btest_sparse"
	if [ -n "$COMPLETE_TESTS" ]; then
		TESTS_B_SET="$TESTS_B_SET btest_cocci btest_smatch"
	fi
	# Run the following in parallel
	TESTS_C_SET="ftest_check_kdoc"
	if [ $UBOOT_TESTING -eq 0 ]; then
		TESTS_C_SET="$TESTS_C_SET ftest_check_includes ftest_check_headerdeps"
	fi

	echo "Tests to run on C files(parallel): $TESTS_C_SET"
	echo "Tests to run on Patch: $TESTS_P_SET"
	echo "Tests to run on Build: $TESTS_B_SET"

	run_test start $TEST_DIR defconfig
	# run twice - we just want end build errors..
	run_test start $TEST_DIR btest_kbuild $ofiles
	run_test start $TEST_DIR btest_kbuild $ofiles

	for test_s in $TESTS_B_SET
	do
		run_test start $TEST_DIR $test_s $ofiles
	done

	PIDS=""
	for test_s in $TESTS_C_SET
	do
		run_test start $TEST_DIR $test_s $cfiles &
		PIDS="$PIDS $!"
	done

	# wait for all to come back
	echo Waiting for test PIDs: $PIDS
	for pid in $PIDS
	do
		wait $pid
	done
	PIDS=""

	for test_s in $TESTS_P_SET
	do
		run_test_dummy start $TEST_DIR $test_s $patch
		run_test end $TEST_DIR $test_s $patch
	done

	run_test end $TEST_DIR defconfig
	# run twice - we just want end build errors..
	run_test end $TEST_DIR btest_kbuild $ofiles
	run_test end $TEST_DIR btest_kbuild $ofiles

	for test_s in $TESTS_B_SET
	do
		run_test end $TEST_DIR $test_s $ofiles
	done

	PIDS=""
	for test_s in $TESTS_C_SET
	do
		run_test end $TEST_DIR $test_s $cfiles &
		PIDS="$PIDS $!"
	done

	# wait for all to come back
	echo Waiting for test PIDs: $PIDS
	for pid in $PIDS
	do
		wait $pid
	done
	PIDS=""
}

report_patch() {
	Subject=`grep "^Subject" $1`
	log_marker "::test results START " `basename $1` "::"
	log_me "Subject: $Subject"
	echo -en "\n"`basename $1` "Tests: ">>$LOG_SUMMARY_FILE
	report_tests defconfig btest_kbuild $TESTS_C_SET $TESTS_B_SET $TESTS_P_SET
	log_marker "::test results END" `basename $1` "::"
}

###################
# Cleanup handler
on_exit() {
	echo -e "\e[0mCleaning up..."

	if [ x != x"$PIDS" ]; then
		echo "Killing $PIDS"
		killall $PIDS 2>/dev/null
	fi

	if [ -n "$DEBUG_MODE" ]; then
		return 0;
	fi
	if [ -f "$TEST_DIR/.config" ]; then
		echo "restoring .config"
		cp $TEST_DIR/.config .config
	fi
	if [ -n "$TEST_DIR" -a -d "$TEST_DIR" ]; then
		echo "Removing temp dir"
		rm -rf $TEST_DIR 2>/dev/null
	fi
	if [ -n "$CURRENT_BRANCH" ]; then
		echo "Restoring to $CURRENT_BRANCH branch"
		git reset --hard 2>/dev/null
		git checkout $CURRENT_BRANCH 2>/dev/null
	fi
	if [ -n "$TEST_BRANCH_NAME" ]; then
		bexists=`git branch|grep "$TEST_BRANCH_NAME" 2>/dev/null`
		if [ -n "$bexists" ]; then
			echo "Cleaning up testing branch"
			git branch -D $TEST_BRANCH_NAME
		fi
	fi
}

###################
# Logging stuff
log_marker() {
	MARKER_STRING="================"
	if [ "$*" ]; then
		echo "$MARKER_STRING">>$LOG_FILE
		echo "$*">>$LOG_FILE
		echo -e "$MARKER_STRING\n">>$LOG_FILE
	else
		echo -e "$MARKER_STRING\n\n">>$LOG_FILE
	fi
}

log_me() {
	echo "$*">>$LOG_FILE
}

logs_me() {
	echo  -e "$*">>$LOG_SUMMARY_FILE
}

report_tests() {
	TESTS=$*
	PASS=1
	for test in $TESTS
	do
		start_log=$TEST_DIR/$test-start
		end_log=$TEST_DIR/$test-end
		diff_log=$TEST_DIR/$test-diff
		if [ -f $start_log -a -f $end_log ]; then
			diff -purN  $start_log $end_log > $diff_log
		fi
	done
	FAIL_TEST=""
	PASS_TEST=""
	for test in $TESTS
	do
		diff_log=$TEST_DIR/$test-diff
		if [ -f $diff_log ]; then
			size=`stat --format "%s" $diff_log`
			if [ $size -ne 0 ]; then
				log_me "$test FAILED?"
				PASS=0
				FAIL_TEST="$FAIL_TEST $test"
			else
				PASS_TEST="$PASS_TEST $test"
			fi
		fi
	done
	if [ $PASS -eq 1 ]; then
		log_me Passed: $TESTS
	else 
		for test in $TESTS
		do
			diff_log=$TEST_DIR/$test-diff
			if [ -f $diff_log ]; then
				size=`stat --format "%s" $diff_log`
				if [ $size -ne 0 ]; then
					log_marker "$test results:"
					cat $diff_log >>$LOG_FILE
				fi
			fi
		done
	fi
	if [ -z "$FAIL_TEST" ]; then
		logs_me "Passed(ALL)"
	else
		logs_me "Failed"
		logs_me "\tFail tests: $FAIL_TEST"
	fi

}

report_end() {

	log_marker "CPUS used: $CPUS"
	log_marker "Application versions"
	for app in $APPS_NEEDED
	do
		echo "version of $app:" >>$LOG_FILE
		(which $app 2>&1) >> $LOG_FILE
		($app --version 2>&1) >> $LOG_FILE
		echo >>$LOG_FILE
	done
	log_marker
	END_DATE=`date`
	END_SEC=`date "+%s"`
	DELTA=`expr $END_SEC - $START_SEC`
	log_marker "Test duration: $DELTA seconds (Started $START_DATE, ended $END_DATE)"
	if [ -f "$LOG_SUMMARY_FILE" ]; then
		echo -e "\e[106m\e[4m\e[1mTest Summary:\e[0m"
		fail=0
		while read ln
		do
			empty=`echo $ln |wc -c`
			# Colored reporting to ensure people dont miss errors
			# See http://misc.flogisoft.com/bash/tip_colors_and_formatting
			if [ $empty -gt 2 ]; then
				pass=`echo $ln|grep "Passed(ALL)"`
				if [ -z "$pass" ]; then
					# Red back, white foreground
					echo -e "\e[1m\e[97m\e[101m$ln\e[0m"
					fail=1
				else
					# Green back, white foreground
					echo -e "\e[1m\e[97m\e[102m$ln\e[0m"
				fi
			fi
		done <$LOG_SUMMARY_FILE
		echo "***************** DETAILED RESULTS *********">>$LOG_SUMMARY_FILE
		cat "$LOG_FILE">>$LOG_SUMMARY_FILE
		mv $LOG_SUMMARY_FILE $LOG_FILE
		echo -ne "\e[96m\e[40mComplete report is available here: "
		if [ $fail -eq 1 ]; then
			echo -e "\e[92m\e[41m\e[5m$LOG_FILE\e[0m"
		else
			echo -e "\e[97m\e[42m\e[5m$LOG_FILE\e[0m"
		fi
	fi
}

###################
# Lets see if we can recommend any missing apps
check_missing_application() {
	APPS_MISSING=""
	for i in $APPS_NEEDED
	do
		which "$i" > /dev/null
		if [ $? -ne 0 ]; then
			APPS_MISSING="$APPS_MISSING $i"
		fi
	done
	if [ -n "$APPS_MISSING" ]; then
		return 2
	fi
	return 0
}

recommend_missing_application() {
	check_missing_application
	if [ -n "$APPS_MISSING" ]; then
		echo "Missing Applications in system: $APPS_MISSING" >&2
		# Lets see if we can recommend an application
		if [ -x /usr/lib/command-not-found ]; then
			for i in $APPS_MISSING
			do
				/usr/lib/command-not-found --no-failure-msg $i
			done
		fi
		return 2
	fi
	return 0
}

APP_NAME=$0
###################
# Help
usage() {
	echo "$*" >&2
	echo "Usage: $APP_NAME [-d] [-j CPUs] [-B build_target] [-T tmp_dir_base] [-l logfile] [-C] [-c defconfig_name] [-n N][-1..9]|[-p patch_dir]|[-b base_branch [-t head_branch]] -U" >&2
	echo -e "\t-d: if not already defined, use CROSS_COMPILE=$DEF_CROSS_COMPILE, ARCH=$DEF_ARCH, and builds for '$KP_TARGETS $DEF_BUILDTARGETS' build targets" >&2
	echo -e "\t-j CPUs: override default CPUs count with build (default is $DEF_CPUS)" >&2
	echo -e "\t-B build_target: override default build target and use provided build_target" >&2
	echo -e "\t-T temp_dir_base: temporary directory base (default is $DEFAULT_TMPDIR)" >&2
	echo -e "\t-l logfile: report file (defaults to $DEFAULT_LOG)" >&2
	echo -e "\t-C: run Complete tests(WARNING: could take significant time!)" >&2
	echo -e "\t-c defconfig:_name (default uses current .config + oldconfig)" >&2
	echo -e "\t-[1..9]: test the tip of current branch (1 to 9 number of patches)" >&2
	echo -e "\t-n N: test the tip of current branch with 'N' number of patches" >&2
	echo -e "\t-p patch_dir: which directory to take patches from (expects sorted in order)" >&2
	echo -e "\t-b base_branch: test patches from base_branch" >&2
	echo -e "\t-t test_branch: optionally used with -b, till head branch, if not provided, along with -b, default will be tip of current branch" >&2
	echo -e "\t-U : Do u-boot basic sanity tests" >&2
	echo -e "\t-m : maximum line length number to be passed on to checkpatch.pl" >&2
	echo >&2
	echo "NOTE: only one of -1, -c, -p or (-b,-t) should be used - but at least one of these should be used" >&2
	echo "NOTE: cannot have a diff pending OR be on a dangling branch base_branch should exist as well" >&2
	echo >&2
	echo "Example usage 1: verify last commmitted patch" >&2
	echo -e "\t $APP_NAME -1" >&2
	echo "Example usage 2: verify on top of current branch patches from location ~/tmp/test-patches" >&2
	echo -e "\t $APP_NAME -p ~/tmp/test-patches" >&2
	echo "Example usage 3: verify *from* branch 'base_branch' till current branch" >&2
	echo -e "\t $APP_NAME -b base_branch" >&2
	echo "Example usage 4: verify from current branch, all patches *until* 'test_branch'" >&2
	echo -e "\t $APP_NAME -t test_branch" >&2
	echo "Example usage 5: verify from branch, all patches from 'base_branch' until 'test_branch'" >&2
	echo -e "\t $APP_NAME -b base_branch -t test_branch" >&2
	echo "Example usage 6: verify from branch Complete tests, all patches from 'base_branch' until 'test_branch'" >&2
	echo -e "\t $APP_NAME -b base_branch -t test_branch -C" >&2
	echo >&2
	echo "Example for usage 7: on a native x86 build using make, gcc and bzImage, 1 patch" >&2
	echo -e "\t $APP_NAME -B bzImage -1" >&2
	echo "Example for usage 7: on a cross_compiled ARM build using defaults, 1 patch" >&2
	echo -e "\t $APP_NAME -d -1" >&2
	echo "Example for usage 8: on a cross_compiled ARM build using defaults,15 patches" >&2
	echo -e "\t $APP_NAME -d -n 15" >&2
	check_missing_application
	if [ $? -ne 0 ]; then
		recommend_missing_application
	fi
}

ORIDE=0
while getopts "n:j:c:T:B:l:p:b:t:m:123456789CdDU" opt; do
	case $opt in
	j)
		CPUS=$OPTARG
	;;
	B)
		export KP_TARGETS="$OPTARG"
		ORIDE=1
	;;
	d)
		if [ -z "$CROSS_COMPILE" ]; then
			export CROSS_COMPILE=$DEF_CROSS_COMPILE
		fi
		if [ -z "$ARCH" ]; then
			export ARCH=$DEF_ARCH
		fi
		if [ -z "$KP_TARGETS" -a $ORIDE -eq 0 ]; then
			export KP_TARGETS="$KP_TARGETS $DEF_BUILDTARGETS"
		fi
	;;
	U)
		DEF_BUILDTARGETS=""
		if [ -n "$KP_TARGETS" -a $ORIDE -eq 0 ]; then
			export KP_TARGETS=""
		fi
		if [ -n "$COMPLETE_TESTS" ]; then
			usage "Cannot run complete tests yet on u-boot"
			exit 1
		fi
		UBOOT_TESTING=1
	;;
	D)
		DEBUG_MODE=1
	;;
	c)
		DEFCONFIG=$OPTARG
	;;
	l)
		LOG_FILE=$OPTARG
	;;
	T)
		TEST_B_DIR=$OPTARG
		if [ ! -d "$TEST_B_DIR" ]; then
			usage "$TEST_B_DIR does not exist"
			exit 1
		fi
		if [ ! -w "$TEST_B_DIR" ]; then
			usage "$TEST_B_DIR is not writable?"
			exit 1
		fi
	;;
	C)
		COMPLETE_TESTS=1
		APPS_NEEDED="$APPS_NEEDED smatch spatch"
		MODULES=modules
		if [ "$UBOOT_TESTING" -eq 1 ]; then
			usage "Cannot run complete tests yet on u-boot"
			exit 1
		fi
	;;
	[1-9])
		TEST_TOP=yes
		if [ -n "$PATCH_DIR" -o -n "$BASE_BRANCH" -o -n "$TEST_BRANCH" ]; then
			usage "cannot use -$opt with other options"
			exit 1;
		fi
		PATCHCOUNT=$opt
	;;
	n)
		TEST_TOP=yes
		if [ -n "$PATCH_DIR" -o -n "$BASE_BRANCH" -o -n "$TEST_BRANCH" ]; then
			usage "cannot use -n with other options"
			exit 1;
		fi
		PATCHCOUNT=$OPTARG
		if [ x"$PATCHCOUNT" == x0 ]; then
			usage "Hey! Do your own '0' patch testing!!!"
			exit 1;
		fi
	;;
	p)
		PATCH_DIR=$OPTARG
		if [ -n "$TEST_TOP" -o -n "$BASE_BRANCH" -o -n "$TEST_BRANCH" ]; then
			usage "cannot use -p with other options"
			exit 1;
		fi
		if [ ! -d "$PATCH_DIR" ]; then
			usage "Patch Directory $PATCH_DIR does not exist?"
			exit 1;
		fi
		PATCHES=`ls $PATCH_DIR/*.patch|grep -v "$COVER_LETTER"|xargs realpath`
		PATCHCOUNT=`ls $PATCHES|wc -l`
		if [ $PATCHCOUNT -eq 0 ]; then
			usage "Patch directory $PATCH_DIR has no patches?"
			exit 1;
		fi
	;;
	b)
		BASE_BRANCH=$OPTARG
		if [ -n "$TEST_TOP" -o -n "$PATCH_DIR" ]; then
			usage "cannot use -b with other options"
			exit 1;
		fi
	;;
	t)
		TEST_BRANCH=$OPTARG
		if [ -n "$TEST_TOP" -o -n "$PATCH_DIR" ]; then
			usage "cannot use -t with other options"
			exit 1;
		fi
		CHECK=`git branch|grep $TEST_BRANCH 2>/dev/null`
		if [ -z "$CHECK" ]; then
			usage "Test branch $TEST_BRANCH does not exist?"
			exit 1
		fi
	;;
	m)
		LINE_LENGTH=$OPTARG
	;;
	\?)
		usage "Invalid option: -$OPTARG"
		exit 1
	;;
	:)
		usage "Option -$OPTARG requires an argument."
		exit 1
	;;
	esac
done

if [ -z "$TEST_BRANCH" -a -z "$BASE_BRANCH" -a -z "$PATCH_DIR" -a -z "$TEST_TOP" ]; then
	usage "Need at least 1 test type"
	exit 2
fi

check_missing_application
if [ $? -ne 0 ]; then
	usage "Missing apps"
	exit 2
fi

if [ -z "$CPUS" ]; then
	CPUS=$DEF_CPUS
fi

if [ -z "$LOG_FILE" ]; then
	LOG_FILE=$DEFAULT_LOG
fi

if [ -z "$TEST_B_DIR" ]; then
	TEST_B_DIR=$DEFAULT_TMPDIR
fi
KDIR=`pwd`

CURRENT_BRANCH=`git branch | grep '^*' | cut -d " " -f 2`
# if we have base or testing branch missing, populate the other as the current branch
if [ -n "$TEST_BRANCH" -a -z "$BASE_BRANCH" ]; then
	BASE_BRANCH=$CURRENT_BRANCH
fi

if [ -n "$BASE_BRANCH" -a -z "$TEST_BRANCH" ]; then
	TEST_BRANCH=$CURRENT_BRANCH
fi
if [ -n "$TEST_BRANCH" -a "$TEST_BRANCH" == "$BASE_BRANCH" ]; then
	usage "Test branch and base branch are the same '$TEST_BRANCH'.. Hmm.. not sleeping lately?"
	exit 3
fi

if [ ! -e ".config" -a -z "$DEFCONFIG" ]; then
	usage "No default .config exists.."
	exit 3
fi

# lets do some basic verification 
gdiff=`git diff`
if [ -n "$gdiff" ]; then
	usage "git diff returned data.. you may want to do git reset --hard or stash changes"
	exit 3
fi

if [ "$CURRENT_BRANCH" = "(no" ]; then
	usage "You are currently on a dangling branch - please checkout a branch to proceed"
	exit 3
fi

TEST_BRANCH_NAME=kernel-patch-verify.$RANDOM
TEST_DIR=$TEST_B_DIR/$TEST_BRANCH_NAME
PATCHD=$TEST_DIR/patches
LOG_SUMMARY_FILE=$TEST_DIR/summary

# NOW, hook on. cleanup.. we are about to start doing serious stuff.
trap on_exit EXIT SIGINT SIGTERM

mkdir -p $TEST_DIR $PATCHD
cp .config $TEST_DIR/.config 2>/dev/null
SMATCH=$TEST_DIR/smatch
echo -e '#!/bin/bash\nsmatch -p=kernel $@'> $SMATCH
chmod +x $SMATCH

# First create a list of patches to test..
if [ -n "$TEST_TOP" ]; then
	if ! [[ x$PATCHCOUNT =~ ^x[0-9]+$ ]] ; then
		usage "error: requested number of patches '$PATCHCOUNT' Not a number"
		exit 4
	fi
	git format-patch --no-cover-letter -M -C -o $PATCHD -$PATCHCOUNT >/dev/null
	git checkout -b $TEST_BRANCH_NAME
	git reset --hard HEAD~$PATCHCOUNT
fi

if [ -n "$PATCHES" ]; then
	cp -rf $PATCHES $PATCHD
	git checkout -b $TEST_BRANCH_NAME
fi

if [ -n "$TEST_BRANCH" ]; then
	git format-patch --no-cover-letter -M -C -o $PATCHD $BASE_BRANCH..$TEST_BRANCH >/dev/null
	PATCHES=`realpath $PATCHD/*.patch|grep -v "$COVER_LETTER"`
	PATCHCOUNT=`ls $PATCHES|wc -l`
	if [ $PATCHCOUNT -eq 0 ]; then
		usage "$BASE_BRANCH..$TEST_BRANCH generated no patches!"
		exit 4;
	fi
	git branch $TEST_BRANCH_NAME $BASE_BRANCH >/dev/null
	git checkout $TEST_BRANCH_NAME
fi

if [ -e "$LOG_FILE" ]; then
	echo "$LOG_FILE exists, taking a backup"
	mv $LOG_FILE $LOG_FILE.bak
fi

if [ -n "$ARCH" ]; then
	KP_PARAMS="$KP_PARAMS ARCH=$ARCH"
fi

if [ -n "$CROSS_COMPILE" ]; then
	KP_PARAMS="$KP_PARAMS CROSS_COMPILE=$CROSS_COMPILE"
fi

START_DATE=`date`
START_SEC=`date "+%s"`

#=========== MAIN TEST TRIGGER LOOP =========
tests_start

PATCHES=`realpath $PATCHD/*.patch|grep -v "$COVER_LETTER"`
PATCHCOUNT=`ls $PATCHES|wc -l`
PATCH_NUM=1
EST_TOTAL="unknown"
ETA_REMAIN="unknown"
DELTAP="unknown"
STARTP_SEC=`date "+%s"`
for patch in $PATCHES
do
	echo "Testing Patch ($PATCH_NUM/$PATCHCOUNT):" `basename $patch` "$DELTAP seconds elapsed, estimated: remaining $ETA_REMAIN  / total $EST_TOTAL seconds"
	test_patch $patch
	report_patch $patch
	NOW_SEC=`date "+%s"`
	DELTAP=`expr $NOW_SEC - $STARTP_SEC`
	AVG=`expr $DELTAP / $PATCH_NUM`
	EST_TOTAL=`expr $AVG \* $PATCHCOUNT`
	ETA_REMAIN=`expr $EST_TOTAL - $DELTAP`
	PATCH_NUM=`expr $PATCH_NUM + 1`
done

tests_end
report_tests_end
report_end

#=========== COMPLETE - let the cleanup handler clean things =========
exit 0
